{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd09164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5",
   "display_name": "Python 3.7.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Conv2D, Dense, Flatten, MaxPooling2D, TimeDistributed, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "MOMENTUM = 0.9\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 5\n",
    "\n",
    "TRAIN_DIR = './Peliculas/Data/train/'\n",
    "TEST_DIR = './Peliculas/Data/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4195/4195 [00:20<00:00, 207.71it/s]\n",
      "100%|██████████| 4410/4410 [00:18<00:00, 243.31it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = []\n",
    "for folder in os.listdir(TRAIN_DIR):\n",
    "    for files in tqdm(os.listdir(os.path.join(TRAIN_DIR, folder))):\n",
    "        path = os.path.join(os.path.join(TRAIN_DIR, folder), files)\n",
    "        image = cv2.resize(cv2.imread(path), (IMG_SIZE, IMG_SIZE))\n",
    "        if folder == 'fight':\n",
    "            TRAIN_DATA.append([np.array(image), [1, 0]])\n",
    "        elif folder == 'nofight':\n",
    "            TRAIN_DATA.append([np.array(image), [0, 1]])\n",
    "\n",
    "shuffle(TRAIN_DATA)\n",
    "np.save('movie_training_data.npy', TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 496/496 [00:02<00:00, 219.26it/s]\n",
      "100%|██████████| 539/539 [00:01<00:00, 274.18it/s]\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA = []\n",
    "for folder in os.listdir(TEST_DIR):\n",
    "    for files in tqdm(os.listdir(os.path.join(TEST_DIR, folder))):\n",
    "        path = os.path.join(os.path.join(TEST_DIR, folder), files)\n",
    "        image = cv2.resize(cv2.imread(path), (IMG_SIZE, IMG_SIZE))\n",
    "        if folder == 'fight':\n",
    "            TEST_DATA.append([np.array(image), [1, 0]])\n",
    "        elif folder == 'nofight':\n",
    "            TEST_DATA.append([np.array(image), [0, 1]])\n",
    "\n",
    "shuffle(TEST_DATA)\n",
    "np.save('movie_testing_data.npy', TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    dataset = []\n",
    "    limit = 0\n",
    "\n",
    "\n",
    "    for file in tqdm(os.listdir('./Peliculas/Dataframes/')):\n",
    "        path = os.path.join('./Peliculas/Dataframes/', file)\n",
    "        image = cv2.resize(cv2.imread(path), (IMG_SIZE, IMG_SIZE))\n",
    "        if limit < 4791:\n",
    "            dataset.append([image, np.array([1, 0])])\n",
    "        elif limit >= 4791:\n",
    "            dataset.append([image, np.array([0, 1])])\n",
    "        \n",
    "        limit += 1 \n",
    "            \n",
    "    \n",
    "    shuffle(dataset)\n",
    "    np.save('movie_data.npy', dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9841/9841 [01:10<00:00, 140.10it/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "data = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('movie_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]fights\n",
      "./Peliculas/Videos/fights\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6lylwdcz\\opencv\\modules\\imgproc\\src\\resize.cpp:3929: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-507ee84befca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'fight'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6lylwdcz\\opencv\\modules\\imgproc\\src\\resize.cpp:3929: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "DATA_SET = [] \n",
    "#for folders in os.listdir('./Peliculas/Data/'+str(folders)): \n",
    "folder_path = './Peliculas/Videos/' \n",
    "for files in tqdm(os.listdir(folder_path)):     \n",
    "    print(files)     \n",
    "    path = os.path.join(folder_path, files)     \n",
    "    print(path)     \n",
    "    for a in os.listdir(path):  \n",
    "        paths= os.path.join(path,a)         \n",
    "        image = cv2.resize(cv2.imread(paths), (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        if files == 'fight':\n",
    "            DATA_SET.append([np.array(image), [1, 0]])         \n",
    "        elif files == 'nofight':             \n",
    "            DATA_SET.append([np.array(image), [0, 1]])  \n",
    "            \n",
    "shuffle(DATA_SET) \n",
    "np.save('movie_data_1.npy', DATA_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_v3 (Functional)    (None, 5, 5, 2048)        21802784  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2048)              0         \n_________________________________________________________________\ndense (Dense)                (None, 2)                 4098      \n=================================================================\nTotal params: 21,806,882\nTrainable params: 4,098\nNon-trainable params: 21,802,784\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "add_model = Sequential()\n",
    "add_model.add(base_model)\n",
    "add_model.add(GlobalAveragePooling2D())\n",
    "add_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model = add_model\n",
    "model.compile(loss='categorical_crossentropy', optimizer = optimizers.SGD(lr=LR, momentum = 0.9), metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "139/139 [==============================] - 29s 198ms/step - loss: 0.6715 - accuracy: 0.6740 - val_loss: 0.5598 - val_accuracy: 0.7574\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 26s 190ms/step - loss: 0.5002 - accuracy: 0.7894 - val_loss: 0.4639 - val_accuracy: 0.8152\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 27s 193ms/step - loss: 0.4345 - accuracy: 0.8266 - val_loss: 0.4115 - val_accuracy: 0.8396\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 27s 192ms/step - loss: 0.3938 - accuracy: 0.8471 - val_loss: 0.3790 - val_accuracy: 0.8508\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 27s 194ms/step - loss: 0.3651 - accuracy: 0.8577 - val_loss: 0.3505 - val_accuracy: 0.8589\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 27s 193ms/step - loss: 0.3384 - accuracy: 0.8688 - val_loss: 0.3617 - val_accuracy: 0.8587\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 28s 199ms/step - loss: 0.3202 - accuracy: 0.8728 - val_loss: 0.3437 - val_accuracy: 0.8618\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 28s 199ms/step - loss: 0.3045 - accuracy: 0.8787 - val_loss: 0.3282 - val_accuracy: 0.8638\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 27s 198ms/step - loss: 0.2910 - accuracy: 0.8825 - val_loss: 0.3148 - val_accuracy: 0.8679\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 28s 200ms/step - loss: 0.2791 - accuracy: 0.8868 - val_loss: 0.3030 - val_accuracy: 0.8699\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 48s 220ms/step - loss: 0.2713 - accuracy: 0.8909 - val_loss: 0.2707 - val_accuracy: 0.8801\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 28s 203ms/step - loss: 0.2618 - accuracy: 0.8951 - val_loss: 0.2602 - val_accuracy: 0.8862\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 28s 200ms/step - loss: 0.2531 - accuracy: 0.8984 - val_loss: 0.2519 - val_accuracy: 0.8882\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 28s 200ms/step - loss: 0.2454 - accuracy: 0.9014 - val_loss: 0.2464 - val_accuracy: 0.8882\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 28s 203ms/step - loss: 0.2382 - accuracy: 0.9063 - val_loss: 0.2379 - val_accuracy: 0.8974\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 28s 203ms/step - loss: 0.2318 - accuracy: 0.9088 - val_loss: 0.2329 - val_accuracy: 0.9106\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 28s 202ms/step - loss: 0.2261 - accuracy: 0.9118 - val_loss: 0.2275 - val_accuracy: 0.9146\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 28s 204ms/step - loss: 0.2206 - accuracy: 0.9155 - val_loss: 0.2225 - val_accuracy: 0.9187\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 28s 201ms/step - loss: 0.2157 - accuracy: 0.9179 - val_loss: 0.2183 - val_accuracy: 0.9228\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 28s 204ms/step - loss: 0.2110 - accuracy: 0.9202 - val_loss: 0.2137 - val_accuracy: 0.9207\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 93s 300ms/step - loss: 0.2052 - accuracy: 0.9232 - val_loss: 0.2227 - val_accuracy: 0.9228\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 33s 236ms/step - loss: 0.2011 - accuracy: 0.9245 - val_loss: 0.2190 - val_accuracy: 0.9268\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 25s 180ms/step - loss: 0.1975 - accuracy: 0.9272 - val_loss: 0.2157 - val_accuracy: 0.9299\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 25s 182ms/step - loss: 0.1940 - accuracy: 0.9277 - val_loss: 0.2127 - val_accuracy: 0.9299\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 25s 183ms/step - loss: 0.1907 - accuracy: 0.9290 - val_loss: 0.2099 - val_accuracy: 0.9299\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 49s 324ms/step - loss: 0.1886 - accuracy: 0.9320 - val_loss: 0.1968 - val_accuracy: 0.9268\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 26s 185ms/step - loss: 0.1855 - accuracy: 0.9343 - val_loss: 0.1948 - val_accuracy: 0.9268\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 26s 186ms/step - loss: 0.1828 - accuracy: 0.9354 - val_loss: 0.1927 - val_accuracy: 0.9289\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 26s 186ms/step - loss: 0.1800 - accuracy: 0.9364 - val_loss: 0.1902 - val_accuracy: 0.9299\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 26s 187ms/step - loss: 0.1774 - accuracy: 0.9382 - val_loss: 0.1889 - val_accuracy: 0.9319\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 26s 189ms/step - loss: 0.1796 - accuracy: 0.9363 - val_loss: 0.1461 - val_accuracy: 0.9512\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 27s 192ms/step - loss: 0.1772 - accuracy: 0.9378 - val_loss: 0.1448 - val_accuracy: 0.9492\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 27s 192ms/step - loss: 0.1748 - accuracy: 0.9390 - val_loss: 0.1429 - val_accuracy: 0.9512\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 27s 195ms/step - loss: 0.1729 - accuracy: 0.9405 - val_loss: 0.1413 - val_accuracy: 0.9512\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 27s 195ms/step - loss: 0.1707 - accuracy: 0.9406 - val_loss: 0.1396 - val_accuracy: 0.9553\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 27s 192ms/step - loss: 0.1677 - accuracy: 0.9421 - val_loss: 0.1477 - val_accuracy: 0.9583\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 27s 194ms/step - loss: 0.1659 - accuracy: 0.9426 - val_loss: 0.1461 - val_accuracy: 0.9553\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 27s 194ms/step - loss: 0.1643 - accuracy: 0.9431 - val_loss: 0.1448 - val_accuracy: 0.9563\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 27s 195ms/step - loss: 0.1623 - accuracy: 0.9432 - val_loss: 0.1443 - val_accuracy: 0.9563\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 27s 192ms/step - loss: 0.1605 - accuracy: 0.9452 - val_loss: 0.1437 - val_accuracy: 0.9583\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 27s 198ms/step - loss: 0.1578 - accuracy: 0.9476 - val_loss: 0.1542 - val_accuracy: 0.9492\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 27s 194ms/step - loss: 0.1559 - accuracy: 0.9481 - val_loss: 0.1531 - val_accuracy: 0.9492\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 27s 193ms/step - loss: 0.1546 - accuracy: 0.9483 - val_loss: 0.1518 - val_accuracy: 0.9553\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 27s 196ms/step - loss: 0.1529 - accuracy: 0.9486 - val_loss: 0.1510 - val_accuracy: 0.9522\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 27s 195ms/step - loss: 0.1519 - accuracy: 0.9495 - val_loss: 0.1503 - val_accuracy: 0.9492\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 37s 213ms/step - loss: 0.1488 - accuracy: 0.9511 - val_loss: 0.1621 - val_accuracy: 0.9411\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 26s 186ms/step - loss: 0.1474 - accuracy: 0.9529 - val_loss: 0.1616 - val_accuracy: 0.9411\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 41s 293ms/step - loss: 0.1465 - accuracy: 0.9524 - val_loss: 0.1604 - val_accuracy: 0.9421\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 26s 188ms/step - loss: 0.1450 - accuracy: 0.9528 - val_loss: 0.1596 - val_accuracy: 0.9431\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 26s 190ms/step - loss: 0.1439 - accuracy: 0.9535 - val_loss: 0.1587 - val_accuracy: 0.9431\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(10)\n",
    "\n",
    "X = np.array([i[0] for i in data]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "Y = np.array([i[1] for i in data])\n",
    "X = X.astype('float32')/255\n",
    "fold = 0\n",
    "\n",
    "for train, test in kf.split(data):\n",
    "    fold += 1\n",
    "    x_train = X[train]\n",
    "    y_train = Y[train]\n",
    "    x_test = X[test]\n",
    "    y_test = Y[test]\n",
    "\n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), verbose = 1, epochs= 5, batch_size=64)\n",
    "    Model_name = 'movie-model-fold' + str(fold) + '.h5'\n",
    "    #weights_name = 'movie-weight-fold' + str(fold) + '.h5'\n",
    "    model.save(Model_name, overwrite = True, include_optimizer = True)\n",
    "    #model.save_weights(weights_name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1969/1969 [==============================] - 71s 36ms/step - loss: 0.1444 - accuracy: 0.9525\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.1444113701581955, 0.9525454640388489]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "model.evaluate(x = X, y =Y, batch_size= 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in data]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "Y = np.array([i[1] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_v3 (Functional)    (None, 5, 5, 2048)        21802784  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2048)              0         \n_________________________________________________________________\ndense (Dense)                (None, 2)                 4098      \n=================================================================\nTotal params: 21,806,882\nTrainable params: 4,098\nNon-trainable params: 21,802,784\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tf = load_model('./movie-model-fold10.h5')\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1969/1969 [==============================] - 65s 32ms/step - loss: 0.1444 - accuracy: 0.9525\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.14441141486167908, 0.9525454640388489]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model_tf.evaluate(x = X, y =Y, batch_size= 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}